% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/harmonization_fns.R
\name{vectorize_gt_parallel}
\alias{vectorize_gt_parallel}
\title{Parallelized Google Translate Wrapper}
\usage{
vectorize_gt_parallel(
  vector,
  target_language = "en",
  source_language,
  workers = 4,
  chunk_size = 50
)
}
\arguments{
\item{vector}{Character vector to be translated.}

\item{target_language}{Two-letter ISO language code for the translation target.
Defaults to `"en"`.}

\item{source_language}{Two-letter ISO language code for the translation source.
Required.}

\item{workers}{Integer. Number of parallel workers to use. Defaults to `4`.}

\item{chunk_size}{Integer. Number of elements in each chunk (batch) to send per
request. Defaults to `50`.}
}
\value{
A character vector of translated text, in the same order as the input.
}
\description{
This function wraps `polyglotr::google_translate()` and adds support for
parallel processing. The input vector is split into chunks, which are
translated in parallel using `future.apply::future_lapply()`. This can be
useful for large vectors or when API rate limits allow concurrent requests.
}
\details{
The function splits the input vector into chunks of size `chunk_size` and
sends each chunk to `polyglotr::google_translate()`. Each chunk is translated
in parallel across `workers` processes. This approach reduces API overhead
compared to sending one request per element.

Note: Using parallel workers will open multiple sessions. Be mindful of API
usage limits or quotas when increasing `workers`.
}
\examples{
\dontrun{
# Translate a few Spanish phrases into English in parallel
phrases <- c("hola", "buenos días", "¿cómo estás?")
vectorize_gt_parallel(phrases, target_language = "en", source_language = "es")
}

}
